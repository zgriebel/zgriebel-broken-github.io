---
title: "PS 3 Feature Analysis and Modeling"
author: "Zach"
date: '2023-09-24'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(knitr)
library(tidyverse)
library(rpart)
library(caret)
library(dummy)
```

```{r}
cars <- read_csv('ToyotaCorolla.csv')
glimpse(cars)
```

There are 38 Variables(not including ID)

```{r}
unique(cars$HP)
length(cars$HP)
```

I do not know much about cars. There appears to be a few standard values for horse power. However, I will have them as a numeric variable because their values relative to each other are significant.

```{r}
cars <- cars %>%
  select(-Id, -Model, -Mfg_Month, -Cylinders, -Quarterly_Tax) %>%
   
  rename(Age = Age_08_04) %>%
  mutate_at(vars(-one_of(
    c('Price',
      'Age',
      'KM',
      'HP',
      'CC',
      'Weight')
  )), .funs = factor)
```

This removed some irrelevant variables. Then set non numeric values to factors.

```{r}
glimpse(cars)
```

That made the values we want to keep numeric variables and the rest factors. Now lets check them for NAs

```{r}
cars %>%
  select(Price,
      Age,
      KM,
      HP,
      CC,
      Weight) %>%
  summary()
```

There appears to not be any NAs in our numeric features.

```{r}
cars %>%
  keep(is.factor) %>%
  summary()
```

Is appears that there are not any NAs in our factor data as well!

```{r}
cars %>% 
  ggplot(aes(x=Price))+ geom_histogram(aes(y = after_stat(density)), fill = "#AB0000" )+geom_density()+ labs(title = "Price of Corrollas",x = "Price")
```

Price is appropriate for linear regression because we are analyzing the relationship of variables on a quantitative variable.

The graph is a bit skewed right. However around the peak of the histogram the distribution is very normal. Then there is a small portion of the data outside of this normal distribution. Given the still relatively normal shape of the graph I believe it is still appropriate to do linear regression without and transformation.

```{r, fig.width= 30}
cars %>%
  select(Price,
      Age,
      KM,
      HP,
      CC,
      Weight) %>% 
  
  
  gather(-Price, key = "var", value = "value") %>% 
  ggplot(aes(x = value, y = Price)) +
    geom_point() +
    facet_wrap(~ var, scales = "free")
```

`Age` of car and `KM` accumulated on the Car displayed a negative correlation with `Price`. Then `weight` appeared to have a loose correlation with `Price`. My assumption would be the `weight` would play a role in the fuel efficiency which is not a variable in this data set.All of these will be analyed further.

```{r}
library(corrplot)
cars %>%
  keep(is.numeric) %>%
  cor() %>%
  corrplot()
```

`Age` and `KM` have a correlation around -.5 this makes sense. As the older the car gets the more miles it is likely to accumulate. This correlation is not strong enough for us to worry, despicably given our background knowledge of the independent importance of each variable. Additionally weight and age are slightly correlated, not much to worry about, however this would most likely be as technology advances the weight of the cars gradually change. HO

```{r}
# Convert all factors to dummy vars.
car_dum = dummy(cars, int = TRUE)
car_num = cars %>%
  keep(is.numeric)
cars = bind_cols(car_num, car_dum)
rm(car_dum, car_num)
```

```{r}
set.seed(2)
samp = createDataPartition(cars$Price, p = 0.7, list = FALSE)
training = cars[samp, ]
testing = cars[-samp, ]
rm(samp)
```

This seperates are data into training and test data variable

Eleminating one dumy variable each

```{r}

training_ols = training %>%
  select(-Mfg_Year_1998,
         -Fuel_Type_CNG,
         -Met_Color_0,
         -Color_Beige,
         -Automatic_0,
         -Doors_2,
         -Gears_3,
         -Mfr_Guarantee_0,
         -BOVAG_Guarantee_0,
         -Guarantee_Period_13,
         -Guarantee_Period_18,
         -Guarantee_Period_20,
         -Guarantee_Period_24,
         -Guarantee_Period_28,
         -Guarantee_Period_36,
         -ABS_0,
         -Airbag_1_0,
         -Airbag_2_0,
         -Airco_0,
         -Automatic_airco_0,
         -Boardcomputer_0,
         -CD_Player_0,
         -Central_Lock_0,
         -Powered_Windows_0,
         -Power_Steering_0,
         -Radio_0,
         -Mistlamps_0,
         -Sport_Model_0,
         -Backseat_Divider_0,
         -Metallic_Rim_0,
         -Radio_cassette_0,
         -Parking_Assistant_0,
         -Tow_Bar_0
         )
```

```{r}
# We set some parameters to control
# how the tree grows.
ctrl = rpart::rpart.control(cp=0.0, 
                            minbucket = 1,
                            maxdepth = 3)
# We actually grow the tree.
dt = rpart(Price ~ .,
           data = training,
           method = "anova",
           control = ctrl
           )

# Use the tree we grew to get
# predicted values.
pred = predict(dt, training)

# Calculate the RMSE of our tree
# on the training data.
pred.rmse = caret::RMSE(pred = pred, obs = training$Price)

# Print out the error measures
paste("Sample Mean:", mean(training$Price))
paste("Sample Stdev:", sd(training$Price))
paste("Model RMSE:", pred.rmse)
```

RMSE is the average error for each point. The average error for this model using training data is \$1347

```{r}
pred = predict(dt, testing)
pred.rmse = caret::RMSE(pred = pred, obs = testing$Price)
paste("Model RMSE:", pred.rmse)
```

Using testing data the average error is \$1446

```{r}
# Visualize the tree we grew.
rpart.plot::rpart.plot(dt)
```

This tree is not too complex. It demonstrated how important the `Age` is, and then we also find results the support some of the other high important features we found such as `weight` and `KM`

```{r}
# Create empty vectors to store values.
rmse.train = c()
rmse.test = c()
rmse.cv = c()
sim.size <- length(training$Price)
# BEGIN LOOP -----------------------------
for(i in 1:10) {
  # Setup tree growth control parameters
  ctrl = rpart::rpart.control(cp=0.0, 
                            minbucket = 1,
                            maxdepth = i)
  # Grow the tree
  dt = rpart(Price ~ .,
           data = training,
           method = "anova",
           control = ctrl
           )
  
  # Cross validation set
  num_folds = 10
  cv_vals = c()
  fold_seq = seq(1, sim.size, sim.size/num_folds)
  # BEGIN CV LOOP -----------------------------------------------------
  for(j in 1:num_folds) {
    fold_rows = seq(fold_seq[j], fold_seq[j]+(sim.size/num_folds)-1, 1)
    idx = seq(1, sim.size, 1) %in% fold_rows
    current_fold = training[idx, ]
    remaining = training[!idx, ]
    # Grow tree with specified ctrls
    # on the non-heldout folds.
    cv.dt = rpart(Price ~.,
           data = remaining,
           method = "anova",
           control = ctrl
           )
    # Get predictions on hold-out fold.
    pred.cv = predict(cv.dt, current_fold)
    # Evaluate tree performance on hold-out fold
    cv_vals[j] = caret::RMSE(pred = pred.cv, obs = current_fold$Price)
  }
  # END CV LOOP --------------------------------------------------------
  
  # Get predictions on training data
  pred.train = predict(dt, training)
  # Get predictions on testing data
  pred.test = predict(dt, testing)
  # Record model performance for training, testing, and cv
  rmse.train[i] = caret::RMSE(pred = pred.train, obs = training$Price)
  rmse.test[i] = caret::RMSE(pred = pred.test, obs = testing$Price)
  rmse.cv[i] = mean(cv_vals)
}
# END LOOP --------------------------------
```

```{r}
# Visualize Model Error Over Complexity
# -------------------------------------
# Set some colors for our plot.
colors <- c("RMSE Training" = "skyblue", 
            "RMSE Testing" = "coral3",
            "RMSE X-val" = "aquamarine3")
# Create the plot.
depth = c(1:10)
ggplot() +
  geom_line(aes(x = depth, y = rmse.train, color = "RMSE Training"), size = 1.2) +
  geom_line(aes(x = depth, y = rmse.test, color = "RMSE Testing"), size = 1.2) +
  geom_line(aes(x = depth, y = rmse.cv, color = "RMSE X-val"), size = 1.2) +
  geom_vline(xintercept = depth[which(rmse.test == min(rmse.test))],
             color = "darkgrey") +
  geom_text(aes(x=3.75, y=1.85, label="<=== Underfit"), color="darkslategrey") +
  geom_text(aes(x=6.1, y=1.85, label="Overfit ===>"), color="darkslategrey") +
  labs(title = "Prediction Error over Model Complexity",
       subtitle = "RMSE on Training vs X-val vs Testing",
       x = "Complexity (Tree Depth)", 
       y = "Prediction Error",
       color = "Legend") +
  scale_color_manual(values = colors) +
  ggthemes::theme_clean()
```

Looks like the best complexity for this model is 5.

```{r, fig.height= 10}
library(iml)
library(patchwork)
lm_predictor = iml::Predictor$new(dt, data = training)
lm_imp = iml::FeatureImp$new(lm_predictor, loss = "rmse", compare = "ratio", n.repetitions = 1)

plot(lm_imp)

```

`Age` and the other numeric factors appear to be of high importance as suggested by my work earlier. Features of less importance are the weird colors, fuel types, and miscellaneous features.

```{r}
results<-lm_imp$results
results$feature
```

There are so many columns with little impact I cut out the first 50 because most of them has little influence.

```{r}
results_feature <-results$feature
features_to_remove <- results_feature[35:86]
features_to_remove
```

I wanted to check with my own human intuition. I Read up and down these as they all logically made sense to be of little importance, which made me feel better about the modeling.

```{r}
cars<- cars[, -which(names(cars) %in% features_to_remove)]

```

```{r}
training<- training[, -which(names(training) %in% features_to_remove)]
testing <- testing[, -which(names(testing) %in% features_to_remove)]
```

Removing those features here.

```{r}
# Create empty vectors to store values.
rmse.train = c()
rmse.test = c()
rmse.cv = c()
sim.size <- length(training$Price)
# BEGIN LOOP -----------------------------
for(i in 1:10) {
  # Setup tree growth control parameters
  ctrl = rpart::rpart.control(cp=0.0, 
                            minbucket = 1,
                            maxdepth = i)
  # Grow the tree
  dt = rpart(Price ~ .,
           data = training,
           method = "anova",
           control = ctrl
           )
  
  # Cross validation set
  num_folds = 10
  cv_vals = c()
  fold_seq = seq(1, sim.size, sim.size/num_folds)
  # BEGIN CV LOOP -----------------------------------------------------
  for(j in 1:num_folds) {
    fold_rows = seq(fold_seq[j], fold_seq[j]+(sim.size/num_folds)-1, 1)
    idx = seq(1, sim.size, 1) %in% fold_rows
    current_fold = training[idx, ]
    remaining = training[!idx, ]
    # Grow tree with specified ctrls
    # on the non-heldout folds.
    cv.dt = rpart(Price ~.,
           data = remaining,
           method = "anova",
           control = ctrl
           )
    # Get predictions on hold-out fold.
    pred.cv = predict(cv.dt, current_fold)
    # Evaluate tree performance on hold-out fold
    cv_vals[j] = caret::RMSE(pred = pred.cv, obs = current_fold$Price)
  }
  # END CV LOOP --------------------------------------------------------
  
  # Get predictions on training data
  pred.train = predict(dt, training)
  # Get predictions on testing data
  pred.test = predict(dt, testing)
  # Record model performance for training, testing, and cv
  rmse.train[i] = caret::RMSE(pred = pred.train, obs = training$Price)
  rmse.test[i] = caret::RMSE(pred = pred.test, obs = testing$Price)
  rmse.cv[i] = mean(cv_vals)
}
# END LOOP --------------------------------
```

```{r}
# Visualize Model Error Over Complexity
# -------------------------------------
# Set some colors for our plot.
colors <- c("RMSE Training" = "skyblue", 
            "RMSE Testing" = "coral3",
            "RMSE X-val" = "aquamarine3")
# Create the plot.
depth = c(1:10)
ggplot() +
  geom_line(aes(x = depth, y = rmse.train, color = "RMSE Training"), size = 1.2) +
  geom_line(aes(x = depth, y = rmse.test, color = "RMSE Testing"), size = 1.2) +
  geom_line(aes(x = depth, y = rmse.cv, color = "RMSE X-val"), size = 1.2) +
  geom_vline(xintercept = depth[which(rmse.test == min(rmse.test))],
             color = "darkgrey") +
  geom_text(aes(x=3.75, y=1.85, label="<=== Underfit"), color="darkslategrey") +
  geom_text(aes(x=6.1, y=1.85, label="Overfit ===>"), color="darkslategrey") +
  labs(title = "Prediction Error over Model Complexity",
       subtitle = "RMSE on Training vs X-val vs Testing",
       x = "Complexity (Tree Depth)", 
       y = "Prediction Error",
       color = "Legend") +
  scale_color_manual(values = colors) +
  ggthemes::theme_clean()
```

After cutting out over half of the variables the tree was refitted. The numbers did not change dirastically. And the complexity of 5 remained constant.

```{r}
# Create empty vectors to store values.
rmse.train = c()
rmse.test = c()
rmse.cv = c()
sim.size <- length(training$Price)
# BEGIN LOOP -----------------------------
for(i in 1:10) {
  # Setup tree growth control parameters
  ctrl = rpart::rpart.control(cp=0.0, 
                            minbucket = 1,
                            maxdepth = 5)
  # Grow the tree
  dt = rpart(Price ~ .,
           data = training,
           method = "anova",
           control = ctrl
           )
  
  # Cross validation set
  num_folds = 10
  cv_vals = c()
  fold_seq = seq(1, sim.size, sim.size/num_folds)
  # BEGIN CV LOOP -----------------------------------------------------
  for(j in 1:num_folds) {
    fold_rows = seq(fold_seq[j], fold_seq[j]+(sim.size/num_folds)-1, 1)
    idx = seq(1, sim.size, 1) %in% fold_rows
    current_fold = training[idx, ]
    remaining = training[!idx, ]
    # Grow tree with specified ctrls
    # on the non-heldout folds.
    cv.dt = rpart(Price ~.,
           data = remaining,
           method = "anova",
           control = ctrl
           )
    # Get predictions on hold-out fold.
    pred.cv = predict(cv.dt, current_fold)
    # Evaluate tree performance on hold-out fold
    cv_vals[j] = caret::RMSE(pred = pred.cv, obs = current_fold$Price)
  }
  # END CV LOOP --------------------------------------------------------
  
  # Get predictions on training data
  pred.train = predict(dt, training)
  # Get predictions on testing data
  pred.test = predict(dt, testing)
  # Record model performance for training, testing, and cv
  rmse.train[i] = caret::RMSE(pred = pred.train, obs = training$Price)
  rmse.test[i] = caret::RMSE(pred = pred.test, obs = testing$Price)
  rmse.cv[i] = mean(cv_vals)
}
# END LOOP --------------------------------
```

```{r}
rmse.train[1]
rmse.test[1]
```

The cross Validation error using our training data was a RMSE of 1038. This means the average error of the model from the training data was \$1038. The RMSE of the model when using the test data with optimum complexity has an average error of \$1169.

This result tells us our model with real corolla data should preform with an an average error of \$1169.
